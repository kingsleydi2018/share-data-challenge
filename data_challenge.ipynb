{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this take home test, I build an xgboost model to predict whether a listing will receive a booking request for a calendar night. Before diving into the model, let's first take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics, preprocessing\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('TH_data_challenge.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_is_requested</th>\n",
       "      <th>ds_night</th>\n",
       "      <th>ds</th>\n",
       "      <th>id_listing_anon</th>\n",
       "      <th>id_user_anon</th>\n",
       "      <th>m_effective_daily_price</th>\n",
       "      <th>m_pricing_cleaning_fee</th>\n",
       "      <th>dim_market</th>\n",
       "      <th>dim_lat</th>\n",
       "      <th>dim_lng</th>\n",
       "      <th>...</th>\n",
       "      <th>general_market_m_reservation_requests_0_6_ds_night</th>\n",
       "      <th>general_market_m_is_booked_0_6_ds_night</th>\n",
       "      <th>m_available_listings_ds_night</th>\n",
       "      <th>kdt_score</th>\n",
       "      <th>r_kdt_listing_views_0_6_avg_n100</th>\n",
       "      <th>r_kdt_n_active_n100</th>\n",
       "      <th>r_kdt_n_available_n100</th>\n",
       "      <th>r_kdt_m_effective_daily_price_n100_p50</th>\n",
       "      <th>r_kdt_m_effective_daily_price_available_n100_p50</th>\n",
       "      <th>r_kdt_m_effective_daily_price_booked_n100_p50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184274</th>\n",
       "      <td>True</td>\n",
       "      <td>2015-06-29</td>\n",
       "      <td>2015-05-30</td>\n",
       "      <td>d55be0a0-f3c9-4771-9d2f-1f1cb692e32c</td>\n",
       "      <td>8d6018eb-03e5-424c-9eb8-d65a4d244a5b</td>\n",
       "      <td>102.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.753384</td>\n",
       "      <td>-122.500374</td>\n",
       "      <td>...</td>\n",
       "      <td>31.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.177419</td>\n",
       "      <td>6.358163</td>\n",
       "      <td>61</td>\n",
       "      <td>32</td>\n",
       "      <td>95.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184275</th>\n",
       "      <td>True</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>2015-07-18</td>\n",
       "      <td>e3e9dc53-125a-474e-935d-bfb5868c3966</td>\n",
       "      <td>7e3ef9e8-537a-41de-add9-b795e30b721e</td>\n",
       "      <td>160.333333</td>\n",
       "      <td>40</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.765903</td>\n",
       "      <td>-122.427490</td>\n",
       "      <td>...</td>\n",
       "      <td>32.571429</td>\n",
       "      <td>1</td>\n",
       "      <td>2032</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>2.163265</td>\n",
       "      <td>61</td>\n",
       "      <td>24</td>\n",
       "      <td>126.5</td>\n",
       "      <td>135.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184276</th>\n",
       "      <td>True</td>\n",
       "      <td>2015-09-03</td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>cc7e4269-f92a-4b8e-a444-e951f80f7b76</td>\n",
       "      <td>a43e3fae-c169-481a-ad43-121e9ab64fb0</td>\n",
       "      <td>36.084746</td>\n",
       "      <td>30</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.773777</td>\n",
       "      <td>-122.403130</td>\n",
       "      <td>...</td>\n",
       "      <td>38.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>2242</td>\n",
       "      <td>1.559524</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184277</th>\n",
       "      <td>False</td>\n",
       "      <td>2015-10-07</td>\n",
       "      <td>2015-09-07</td>\n",
       "      <td>b5266b83-1dc7-47ba-8d42-dd66037f79b8</td>\n",
       "      <td>41d906f7-2556-4309-a406-f9a7a9e2c30c</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.758934</td>\n",
       "      <td>-122.437300</td>\n",
       "      <td>...</td>\n",
       "      <td>36.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>2631</td>\n",
       "      <td>1.211268</td>\n",
       "      <td>1.852041</td>\n",
       "      <td>96</td>\n",
       "      <td>31</td>\n",
       "      <td>220.0</td>\n",
       "      <td>271.5</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184278</th>\n",
       "      <td>False</td>\n",
       "      <td>2015-12-15</td>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>3ff697ff-2c9c-40dc-b9bb-71e4fc62d4b8</td>\n",
       "      <td>7066dc20-25e5-45e1-88d0-ec897fb4aa94</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>80</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.772460</td>\n",
       "      <td>-122.417060</td>\n",
       "      <td>...</td>\n",
       "      <td>25.857143</td>\n",
       "      <td>1</td>\n",
       "      <td>3561</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.351020</td>\n",
       "      <td>69</td>\n",
       "      <td>27</td>\n",
       "      <td>217.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ]
     },
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "# look at the data\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184279, 46)\n"
     ]
    }
   ],
   "source": [
    "print data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not difficult to figure that the task is a binary classification problem with 45 features. Through diving into the features, we can see that 'ds-night' is equivalent to 'ds-night-day-of-year' and 'ds' + 30 days = 'ds-night'. Therefore, we can remove 'ds-night' and 'ds' as they are redundant. Similarly, 'id-lising-anon' is equivalent to 'is-user-anon', and hence one of them can be removed from the feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter redundant features\n",
    "filters = ['ds_night', 'ds', 'id_user_anon']\n",
    "data = data.drop(filters, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dim_is_requested</th>\n",
       "      <th>id_listing_anon</th>\n",
       "      <th>m_effective_daily_price</th>\n",
       "      <th>m_pricing_cleaning_fee</th>\n",
       "      <th>dim_market</th>\n",
       "      <th>dim_lat</th>\n",
       "      <th>dim_lng</th>\n",
       "      <th>dim_room_type</th>\n",
       "      <th>dim_person_capacity</th>\n",
       "      <th>dim_is_instant_bookable</th>\n",
       "      <th>...</th>\n",
       "      <th>general_market_m_reservation_requests_0_6_ds_night</th>\n",
       "      <th>general_market_m_is_booked_0_6_ds_night</th>\n",
       "      <th>m_available_listings_ds_night</th>\n",
       "      <th>kdt_score</th>\n",
       "      <th>r_kdt_listing_views_0_6_avg_n100</th>\n",
       "      <th>r_kdt_n_active_n100</th>\n",
       "      <th>r_kdt_n_available_n100</th>\n",
       "      <th>r_kdt_m_effective_daily_price_n100_p50</th>\n",
       "      <th>r_kdt_m_effective_daily_price_available_n100_p50</th>\n",
       "      <th>r_kdt_m_effective_daily_price_booked_n100_p50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>184274</th>\n",
       "      <td>True</td>\n",
       "      <td>d55be0a0-f3c9-4771-9d2f-1f1cb692e32c</td>\n",
       "      <td>102.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.753384</td>\n",
       "      <td>-122.500374</td>\n",
       "      <td>Private room</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>31.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>1.177419</td>\n",
       "      <td>6.358163</td>\n",
       "      <td>61</td>\n",
       "      <td>32</td>\n",
       "      <td>95.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184275</th>\n",
       "      <td>True</td>\n",
       "      <td>e3e9dc53-125a-474e-935d-bfb5868c3966</td>\n",
       "      <td>160.333333</td>\n",
       "      <td>40</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.765903</td>\n",
       "      <td>-122.427490</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>32.571429</td>\n",
       "      <td>1</td>\n",
       "      <td>2032</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>2.163265</td>\n",
       "      <td>61</td>\n",
       "      <td>24</td>\n",
       "      <td>126.5</td>\n",
       "      <td>135.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184276</th>\n",
       "      <td>True</td>\n",
       "      <td>cc7e4269-f92a-4b8e-a444-e951f80f7b76</td>\n",
       "      <td>36.084746</td>\n",
       "      <td>30</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.773777</td>\n",
       "      <td>-122.403130</td>\n",
       "      <td>Shared room</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>38.428571</td>\n",
       "      <td>1</td>\n",
       "      <td>2242</td>\n",
       "      <td>1.559524</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184277</th>\n",
       "      <td>False</td>\n",
       "      <td>b5266b83-1dc7-47ba-8d42-dd66037f79b8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.758934</td>\n",
       "      <td>-122.437300</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>36.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>2631</td>\n",
       "      <td>1.211268</td>\n",
       "      <td>1.852041</td>\n",
       "      <td>96</td>\n",
       "      <td>31</td>\n",
       "      <td>220.0</td>\n",
       "      <td>271.5</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184278</th>\n",
       "      <td>False</td>\n",
       "      <td>3ff697ff-2c9c-40dc-b9bb-71e4fc62d4b8</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>80</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>37.772460</td>\n",
       "      <td>-122.417060</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>25.857143</td>\n",
       "      <td>1</td>\n",
       "      <td>3561</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.351020</td>\n",
       "      <td>69</td>\n",
       "      <td>27</td>\n",
       "      <td>217.5</td>\n",
       "      <td>250.0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ]
     },
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "# look at the filtered data\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train an xgboost classifier, the data has to be converted into DMatrix format. The 'object' type is not yet supported by DMatrix, and hence all the 'object' type columns are preprocessed (encoded) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode object types\n",
    "for f in data.columns:\n",
    "    if data[f].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(data[f].values))\n",
    "        data[f] = lbl.transform(list(data[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data and create dummies for categorical variables\n",
    "y_var = 'dim_is_requested'\n",
    "cat_var = ['dim_market', 'dim_room_type', 'dim_is_instant_bookable', 'cancel_policy', \n",
    "           'dim_has_wireless_internet']\n",
    "\n",
    "data = shuffle(data, random_state=0)\n",
    "data = pd.get_dummies(data, columns=cat_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 variables we can either consider them as numerical or categorical: 'ds-night-day-of-week', 'ds-night-day-of-year', 'ds-checkin_gap' and 'ds-checkout-gap'. I tested all the combinations in the cross validation step (explained later in this writeup) and decided to treat them as numerical for higher prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing sets\n",
    "training = data[:int(len(data)*0.8)]\n",
    "testing = data[int(len(data)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147423, 54)\n(36856, 54)\n"
     ]
    }
   ],
   "source": [
    "print training.shape\n",
    "print testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training data into DMatrix format\n",
    "xgdmat = xgb.DMatrix(training[[i for i in training.columns if i != y_var]], training[y_var], \n",
    "                     missing=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters for xgboost and train the xgboost model\n",
    "our_params = {'eta':0.1, 'seed':0, 'subsample':0.8, 'colsample_bytree':0.8, \n",
    "              'objective':'binary:logistic', 'max_depth':10, 'min_child_weight':5, \n",
    "              'alpha':100, 'beta':100}\n",
    "xgb_model = xgb.train(our_params, xgdmat, num_boost_round=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has a small amount of missing values. The reason that I did not deal with these missing values explicitly is because 1) the amount is not significantly large; 2) xgboost has internal functionality to handle missing values and usually performs better than manually filling in missing values with zeros or median values. Internally, xgboost will automatically learn what is the best direction to go when a value is missing. Equivalently, this can be viewed as automatically \"learn\" what is the best imputation value for missing values based on reduction on training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.12%\n"
     ]
    }
   ],
   "source": [
    "# score the test data and compute accuracy of classification\n",
    "y_pred = xgb_model.predict(xgb.DMatrix(testing[[i for i in testing.columns if i != y_var]],\n",
    "                                       testing[y_var], missing=np.nan))\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = metrics.accuracy_score(testing[y_var].values, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good accuracy score on this problem, which we would expect, given 1) the capabilities of the xgboost model; 2) the modest complexity of the problem; and 3) the fact that all the major parameters were tuned via extensive cross validation experiments. To be more specific, I have tested all the combinations of â€˜etaâ€™ being in [0.01, 0.05, 0.1, 0.2], â€˜max-depth-valâ€™ being in [3, 5, 10], â€˜min-child-weightâ€™ being in [1, 3, 5],  alpha regularization being in [1, 50, 100], beta regularization being in [1, 50, 100], and finally number of boosting rounds ranging across [100, 200, 300]. As mentioned in previous steps, I also tested any of the ['ds-night-day-of-week', 'ds-night-day-of-year', 'ds-checkin-gap', 'ds-checkout-gap'] should be treated as categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features:\n\n                                  0\nm_effective_daily_price        1413\ndim_lat                        1050\ndim_lng                         853\nm_available_listings_ds_night   845\nimage_quality_score             796\nprice_booked_most_recent        770\nds_night_day_of_year            725"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# output feature importance\n",
    "print 'Top Features:\\n'\n",
    "print pd.DataFrame(xgb_model.get_fscore(), \n",
    "                   index=[0]).transpose().sort_values(0, ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top features align with our intuition, as price, location, timing and availability are usually the determining factor for such listing bookings. In this case, image quality is also a key feature which makes intuitive sense due to the nature of online shopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Discussions: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Baseline and model improvement: we can certainly start modeling the task using the simplest logistic regression. However, as an experienced data scientist, we should be able to figure out (without implementing and testing) that given the nature of the problem and structure of the data, gradient boosting methods (e.g., xgboost, lightgbm and etc) should be the way to go. What I did in this test is that I implemented the model using some guessed parameters (i.e., educated guess based on experience) and then due to the fact that the size of the data is not so large, I conducted cross validations to finely tune the parameters and improve the model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Identify new features: although many new features can be derived from the given dataset, I think â€˜closeness-to-major-attractionsâ€™ deriving from location data and â€˜ds-night-to-next-holidayâ€™ deriving from time data would be extraordinarily useful. The likelihood if a listing is going be booked on a particular night highly depends on 1) if the listed property is close to some major attractions such as national parks; and 2) if there is any holidays approaching to the predicted booking night."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to use the model and findings: 1) Since the predicted scores indicate how likely a listing is going to be booked, Airbnb can always use these probability scores to improve their offer rankings to drive more click-through and bookings. For example, â€˜probabilityâ€™ * â€˜profitâ€™ is a very good ranking indicator to me. 2) Hinted by the results, certain feature such as pricing, location and image quality are more important than others. Airbnb can always develop site functionalities to â€˜promoteâ€™ listings that have certain advantages. For example, we can promote property A by its location and price, and we can promote property B by the high quality photos and availabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, the given dataset could potentially be used for many other prediction tasks. The most applicable one, I believe, is the demand forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the time limit, above is the immediate approach I came up with. With more time and more understanding on the data, I would definitely like to do more feature engineering and try to compare the results using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}